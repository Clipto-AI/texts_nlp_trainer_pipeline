from zenml import pipeline, step
import subprocess
import argparse
import os
import sys

@step
def prepare_data(source_dir: str):
    print(f"âœ… [prepare_data] æ•°æ®å‡†å¤‡é˜¶æ®µï¼Œæºç›®å½•: {source_dir}")
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ process.py
    process_file = os.path.join(source_dir, "process.py")
    if os.path.exists(process_file):
        print("ğŸ” å‘ç° process.pyï¼Œè°ƒç”¨ preprocess_data.sh è„šæœ¬")
        try:
            # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
            current_dir = os.path.dirname(os.path.abspath(__file__))
            preprocess_script = os.path.join(current_dir, "preprocess_data.sh")
            
            # æ£€æŸ¥ preprocess_data.sh æ˜¯å¦å­˜åœ¨
            if not os.path.exists(preprocess_script):
                raise FileNotFoundError(f"preprocess_data.sh è„šæœ¬ä¸å­˜åœ¨: {preprocess_script}")
            
            # ç¡®ä¿è„šæœ¬æœ‰æ‰§è¡Œæƒé™
            os.chmod(preprocess_script, 0o755)
            
            # è°ƒç”¨ preprocess_data.sh è„šæœ¬
            cmd = ["bash", preprocess_script, source_dir]
            print(f"ğŸš€ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            # ä½¿ç”¨ Popen æ¥å®æ—¶æ˜¾ç¤ºè¾“å‡ºï¼ŒåŒæ—¶æ•è·è¾“å‡º
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, 
                                     text=True, bufsize=1, universal_newlines=True)
            
            output_lines = []
            print("ğŸ“‹ æ•°æ®å¤„ç†è¾“å‡º:")
            print("-" * 50)
            
            # å®æ—¶è¯»å–å¹¶æ˜¾ç¤ºè¾“å‡º
            for line in iter(process.stdout.readline, ''):
                if line:
                    print(line.rstrip())  # å®æ—¶æ˜¾ç¤ºè¾“å‡º
                    output_lines.append(line.rstrip())  # ä¿å­˜åˆ°åˆ—è¡¨
            
            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            return_code = process.wait()
            
            print("-" * 50)
            
            if return_code != 0:
                print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥ï¼Œè¿”å›ç : {return_code}")
                raise RuntimeError("æ•°æ®å¤„ç†å¤±è´¥")
            
            # ä»è¾“å‡ºä¸­æå–ç›®å½•è·¯å¾„
            # æŸ¥æ‰¾ "âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ" è¿™ä¸€è¡Œï¼Œç„¶åå–å‰ä¸¤è¡Œä½œä¸ºç›®å½•è·¯å¾„
            dataset_dir = None
            dataset_eval_dir = None
            
            # ä»åå¾€å‰æŸ¥æ‰¾ "âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ" è¿™ä¸€è¡Œ
            completion_line_index = -1
            for i in range(len(output_lines) - 1, -1, -1):
                if "âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ" in output_lines[i]:
                    completion_line_index = i
                    break
            
            if completion_line_index >= 2:
                # å–å‰ä¸¤è¡Œä½œä¸ºç›®å½•è·¯å¾„
                dataset_eval_dir = output_lines[completion_line_index - 1].strip()
                dataset_dir = output_lines[completion_line_index - 2].strip()
                dataset_eval_dir = dataset_eval_dir.replace("/data", source_dir)
                dataset_dir = dataset_dir.replace("/data", source_dir)
            
            # å¦‚æœæ— æ³•ä»è¾“å‡ºä¸­æå–è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
            if dataset_dir is None or dataset_eval_dir is None:
                print("âš ï¸  æ— æ³•ä»è¾“å‡ºä¸­æå–ç›®å½•è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„")
                dataset_dir = os.path.join(source_dir, "processed_train")
                dataset_eval_dir = os.path.join(source_dir, "processed_eval")
                
            print(f"ğŸ“ è®­ç»ƒæ•°æ®ç›®å½•: {dataset_dir}")
            print(f"ğŸ“ è¯„ä¼°æ•°æ®ç›®å½•: {dataset_eval_dir}")
            return dataset_dir, dataset_eval_dir
            
        except Exception as e:
            print(f"âŒ è°ƒç”¨ preprocess_data.sh å¤±è´¥: {e}")
            raise
    else:
        print("ğŸ“‚ æœªå‘ç° process.pyï¼Œä½¿ç”¨é»˜è®¤ç›®å½•ç»“æ„")
        # é»˜è®¤ä½¿ç”¨ source_dir/train å’Œ source_dir/eval
        dataset_dir = os.path.join(source_dir, "train")
        dataset_eval_dir = os.path.join(source_dir, "eval")
        
        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(dataset_dir):
            print(f"âš ï¸  è­¦å‘Š: è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨: {dataset_dir}")
        if not os.path.exists(dataset_eval_dir):
            print(f"âš ï¸  è­¦å‘Š: è¯„ä¼°æ•°æ®ç›®å½•ä¸å­˜åœ¨: {dataset_eval_dir}")
            
        print(f"ğŸ“ è®­ç»ƒæ•°æ®ç›®å½•: {dataset_dir}")
        print(f"ğŸ“ è¯„ä¼°æ•°æ®ç›®å½•: {dataset_eval_dir}")
        return dataset_dir, dataset_eval_dir

@step
def train_model(dataset_dirs: tuple, training_script: str, training_env: str = None):
    dataset_dir, dataset_eval_dir = dataset_dirs
    print(f"ğŸš€ [train_model] è®­ç»ƒæ•°æ®ç›®å½•: {dataset_dir}")
    print(f"ğŸš€ [train_model] è¯„ä¼°æ•°æ®ç›®å½•: {dataset_eval_dir}")
    print(f"ğŸš€ [train_model] è®­ç»ƒè„šæœ¬: {training_script}")
    
    # å‡†å¤‡ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®¾ç½®
    if "DATASET_DIR" not in env:
        env["DATASET_DIR"] = dataset_dir
        print(f"ğŸ”§ è®¾ç½®ç¯å¢ƒå˜é‡ DATASET_DIR: {dataset_dir}")
    else:
        print(f"ğŸ”§ ä½¿ç”¨ç°æœ‰ç¯å¢ƒå˜é‡ DATASET_DIR: {env['DATASET_DIR']}")
        
    if "DATASET_EVAL_DIR" not in env:
        env["DATASET_EVAL_DIR"] = dataset_eval_dir
        print(f"ğŸ”§ è®¾ç½®ç¯å¢ƒå˜é‡ DATASET_EVAL_DIR: {dataset_eval_dir}")
    else:
        print(f"ğŸ”§ ä½¿ç”¨ç°æœ‰ç¯å¢ƒå˜é‡ DATASET_EVAL_DIR: {env['DATASET_EVAL_DIR']}")
    
    # æ„å»ºè®­ç»ƒå‘½ä»¤
    if training_env:
        print(f"ğŸ”§ ä½¿ç”¨è®­ç»ƒç¯å¢ƒ: {training_env}")
        # å¦‚æœæŒ‡å®šäº† training_envï¼Œä¼ é€’ --training_env å‚æ•°ç»™è®­ç»ƒè„šæœ¬
        cmd = ["bash", training_script, "--training_env", training_env]
    else:
        cmd = ["bash", training_script]
    
    print(f"ğŸš€ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
    
    # ä½¿ç”¨ Popen æ¥å®æ—¶æ˜¾ç¤ºè¾“å‡ºï¼ŒåŒæ—¶æ•è·æœ€åä¸€è¡Œ
    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, 
                             text=True, env=env, bufsize=1, universal_newlines=True)
    
    output_lines = []
    print("ğŸ“‹ è®­ç»ƒè¾“å‡º:")
    print("-" * 50)
    
    # å®æ—¶è¯»å–å¹¶æ˜¾ç¤ºè¾“å‡º
    for line in iter(process.stdout.readline, ''):
        if line:
            print(line.rstrip())  # å®æ—¶æ˜¾ç¤ºè¾“å‡º
            output_lines.append(line.rstrip())  # ä¿å­˜åˆ°åˆ—è¡¨
    
    # ç­‰å¾…è¿›ç¨‹å®Œæˆ
    return_code = process.wait()
    
    print("-" * 50)
    
    if return_code != 0:
        print("âŒ è®­ç»ƒå¤±è´¥ï¼Œè¿”å›ç :", return_code)
        raise RuntimeError("è®­ç»ƒå¤±è´¥")
    
    # ä»è¾“å‡ºä¸­æå–æœ€åä¸€è¡Œä½œä¸ºè¾“å‡ºç›®å½•
    if output_lines and output_lines[-1].strip():
        # å°è¯•ä»æœ€åä¸€è¡Œè·å–è¾“å‡ºç›®å½•
        potential_output = output_lines[-1].strip()
        if os.path.exists(potential_output):
            output_dir = potential_output
        else:
            # ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•
            output_dir = os.path.join(os.getcwd(), "output")
    else:
        # ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•
        output_dir = os.path.join(os.getcwd(), "output")
    
    print(f"ğŸ“ è®­ç»ƒè¾“å‡ºç›®å½•: {output_dir}")
    return output_dir

@step
def evaluate_model(output_dir: str):
    print(f"ğŸ§© [evaluate_model] æ¨¡å‹è¾“å‡ºç›®å½•: {output_dir}")
    print("ğŸ“Š æ¨¡å‹è¯„ä¼°é˜¶æ®µï¼ˆå¾…å®ç°ï¼‰")
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(output_dir):
        print(f"âš ï¸  è­¦å‘Š: è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")
        return "evaluation_failed"
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„è¯„ä¼°é€»è¾‘
    # ä¾‹å¦‚ï¼šåŠ è½½æ¨¡å‹ã€è¿è¡Œè¯„ä¼°è„šæœ¬ç­‰
    print(f"ğŸ“ è¯„ä¼°æ¨¡å‹ï¼Œè¾“å‡ºç›®å½•: {output_dir}")
    
    return "evaluation_done"

@pipeline(enable_cache=False)
def training_pipeline(source_dir: str, training_script: str, training_env: str = None):
    dataset_dirs = prepare_data(source_dir)
    output_dir = train_model(dataset_dirs, training_script, training_env)
    evaluate_model(output_dir)

if __name__ == "__main__":
    print("ğŸŒ€ å¯åŠ¨ ZenML Pipeline ...")
    parser = argparse.ArgumentParser(description="ZenML è®­ç»ƒç®¡é“")
    parser.add_argument("--source_dir", type=str, required=True, 
                       help="æ•°æ®æºç›®å½•è·¯å¾„")
    parser.add_argument("--training_script", type=str, default="run_training.sh",
                       help="è®­ç»ƒè„šæœ¬è·¯å¾„ (é»˜è®¤: run_training.sh)")
    parser.add_argument("--training_env", type=str, default=None,
                       help="è®­ç»ƒç¯å¢ƒè„šæœ¬è·¯å¾„ (å¯é€‰)")
    args = parser.parse_args()
    
    print(f"ğŸ“‚ æ•°æ®æºç›®å½•: {args.source_dir}")
    print(f"ğŸš€ è®­ç»ƒè„šæœ¬: {args.training_script}")
    if args.training_env:
        print(f"ğŸ”§ è®­ç»ƒç¯å¢ƒ: {args.training_env}")
    
    training_pipeline(args.source_dir, args.training_script, args.training_env)
