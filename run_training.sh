#!/usr/bin/env bash
set -euo pipefail

# ============ æ£€æŸ¥ .env æ–‡ä»¶æ˜¯å¦å­˜åœ¨ ============
if [ ! -f training_env ]; then
  echo "âŒ é”™è¯¯: æ‰¾ä¸åˆ° training_env æ–‡ä»¶ï¼Œè¯·åœ¨å½“å‰ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ª training_env æ–‡ä»¶ã€‚"
  echo "   ç¤ºä¾‹ï¼š"
  echo "   WANDB_API_KEY=xxxx"
  echo "   MODEL_NAME=/path/to/model"
  echo "   DATASET_DIR=/path/to/data"
  exit 1
fi

# ============ ä» .env åŠ è½½æ‰€æœ‰ç¯å¢ƒå˜é‡ ============
echo "ğŸ“¦ æ­£åœ¨åŠ è½½ training_env æ–‡ä»¶ ..."
export $(grep -v '^#' training_env | xargs)

# ============ æ£€æŸ¥å…³é”®å˜é‡æ˜¯å¦å­˜åœ¨ ============
REQUIRED_VARS=(MODEL_NAME DATASET_DIR OUTPUT_DIR)
for var in "${REQUIRED_VARS[@]}"; do
  if [ -z "${!var:-}" ]; then
    echo "âŒ é”™è¯¯: ç¯å¢ƒå˜é‡ $var æœªåœ¨ .env ä¸­å®šä¹‰"
    exit 1
  fi
done

# ============ æ‰“å°å½“å‰é…ç½® ============
echo "âœ… ç¯å¢ƒå˜é‡åŠ è½½å®Œæˆï¼š"
echo "  WANDB_PROJECT=${WANDB_PROJECT:-undefined}"
echo "  MODEL_NAME=${MODEL_NAME}"
echo "  DATASET_DIR=${DATASET_DIR}"
echo "  OUTPUT_DIR=${OUTPUT_DIR}"
echo "  EPOCHS=${EPOCHS:-10}"
echo "  BATCH_SIZE=${BATCH_SIZE:-2}"
echo "  LEARNING_RATE=${LEARNING_RATE:-3e-4}"

# ============ å¯åŠ¨è®­ç»ƒ ============
echo "ğŸš€ å¼€å§‹è®­ç»ƒ ..."
apptainer run --bind /DATA_B:/workspace,/DATA_A:/data --nv env/apptainer.sif \
 accelerate launch --config_file default_config.yaml\
  src/train.py \
  --model_name "${MODEL_NAME}" \
  --dataset_dir "${DATASET_DIR}" \
  --epochs "${EPOCHS}" \
  --lora_init_method "${LORA_INIT_METHOD}" \
  --batch_size "${BATCH_SIZE}" \
  --total_max_length "${TOTAL_MAX_LENGTH}" \
  --save_steps "${SAVE_STEPS}" \
  --eval_steps "${EVAL_STEPS}" \
  --output_dir "${OUTPUT_DIR}" \
  --learning_rate "${LEARNING_RATE}" \
  --lora_rank "${LORA_RANK}" \
  --dataset_eval_dir "${DATASET_EVAL_DIR}" \
  --save_total_limit "${SAVE_TOTAL_LIMIT}"

echo "ğŸ‰ è®­ç»ƒå®Œæˆï¼"
